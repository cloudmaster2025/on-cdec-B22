Docker > container > trust > manually > Docker-swarm > to manage containers automatically

Google > Borg > 2015 > Linux Foundation > Kubernetes(k8s) -> open source, very advance,
high secure, free to use

              KUBERNETES [container orchestration tool]
      cluster
      nodes{ Master node, Worker nodes}
      pods  (smallest unit of deployment in k8s)

  Master node components
 1) API server > Route the request
 2) ETCD  > Store the data here
 3) Scheduler > Schedule the nodes(server)
 4) Controller Manager > keep the resources in your desired state
                          (current state = Desired state)

  Worker node components
 1) Kubelette > create the pod and monitor it
 2) container runtime > provide environment to create container inside pod
 3) Kube-proxy > provide the networking (ip-addresses) to the pod

   
Pod > smallest unit of deployment in k8s
    > wrapper in which we create containers 
    > we can create multiple containers in one pod 
    > pod has its own ip address



Objects     -> Cluster
(RESOURCES)  > Node
         > Pod 
         > Namespace
         > service - to expose pod outside and inside  the cluster 
         > Replica set {Earlier -Replication controller}
         > Deployment
         > StatefulSet
         > Daemonset
         > PV and PVC
         > Ingress 
         > Horizontal Pod Autoscalling
         > config map  and secrets variables 


Cluster - on cloud         , on premises,     on server,        webapplcation
          EKS {managed}       Kubeadm          minikube         killercoda {two node cluster}
          AKS                 Kind            {single node}
          GKE                {Self-Managed}



# kubectl {prefix of k8s commands}
# kubectl run <podname> --image=<imagename>   {to create and run the pod}
# kubectl get <resource-name>
# kubectl get node 
# kubectl get pod -o wide  {described details}


* In a pod there are two container one is main and other is side car container

manifest {yaml file}   "Indentation  maintain"
 
vi pod.yaml

apiVersion: "library"
kind:    "Type of resource"
metadata: "data of data"
spec: "service of pod"




apiVersion:  v1   
kind :  Pod       
metadata:  
  name: demopod
  label:
    team: prod
spec:
  containers:
  - name: container1
    image: nginx
  - name: container2
    image: tomcat 
     

after creation of file please apply it

# kubectl apply -f <filename>



*Port no. of container is imp. for communicate with particular container in a pod
*No two container in same pod can have same port
  
        
-->   Intra-pod communication
1) same pod containers connect
2) Use podIP or localhost to connect

#  kubectl exec -it demo-pod -- bash
#  kubectl exec -it -c container2 demo-pod -- bash

-->  Inter-Pod
 
two different pod communication

#kubectl api-resources 
  
Pod get terminate again and again and ip got changed but not label of pod

@@@@@Service@@@@
 type- 1) ClusterIP -  which i used to provide static IP and common/single Ip to the pods, but work only inside         	     		       cluster ((default one))
       
       2) NodePort   -  service resource will generate a random port no.{30k}, and use it with nodeIP       						[nodeIp:nodePort]
       3) LoadBalancer -  when there are multiple nodes

{

apiVersion: v1
kind: Service
metadata: 
  name: my-svc
spec:
  selector:
    team: dev
  ports:
  - port: 80
    targetPort: 80
  type: NodePort

}   


Replication Controller - to keep maintain the no. of pods you create in the running state at a given time

apiVersion: v1
kind: ReplicationController
metadata:
  name: my-rc
  labels:
    app: my-app
spec:
  template:
    metadata:
      name: Pod1
      labels:
        team: test
    spec:
      containers:
      - name: cont1
        image: nginx
  selector:
    team: test
  replicas: 4


*labels are important here
* recreates pod automatically at desired replica
* to delete pod , need to delete rc


Replica-set =>  this work as a set based selector


---> set based labels selector
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-rs
  labels:
    app: my-app
spec:
  template:
    metadata:
      name: Pod1
      labels:
        team: test
        team: prod
        group: dev
    spec:
      containers:
      - name: cont1
        image: nginx
  selector:
    matchExpressions:
      key: ["team","group"]
      values: ["test","dev","prod"]
  replicas: 4

--->
no set based selector

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-rs
  labels:
    app: my-app
spec:
  template:
    metadata:
      name: Pod1
      labels:
        team: dev       
    spec:
      containers:
      - name: cont1
        image: nginx
  selector:
    matchLabels:
      team: dev
  replicas: 4



@@@@Namespace@@@@@


                   

                      @@@Deployment@@@@
> To manage and maintain that no. of pods are kept running on a particular given time.
>  rolling update and rolling back policy{task}"
****{generally use for stateless application like frontend and backend}
    
$Rolling update > in this policy the resource pods get automatically update as per updation in manifest
>>>>>>strategy -> 1) Rolling Update strategy -  first create then destroy  {defaut}
                  2) Recreate   -  first delete all in one go and then create in one go
                  3) Canary deployment   (task)
                  4) Blue-green deployment (task)

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deploy
  labels:
    app: my-app
spec:
  template:
    metadata:
      name: Pod1
      labels:
        team: dev       
    spec:
      containers:
      - name: cont1
        image: nginx
  selector:
    matchLabels:
      team: dev
  replicas: 4
  strategy:
    type: Recreate



            @@@StatefulSet@@@
 game > level 10 > next day > level 0   --- > stateless
 game > level 10 > next day  > level 10    ----> stateful (previous states matter)

 *** it is use for stateful application like database application 
   ** all pods created by ss have there unique identity start from 0----
 


     @@@configMap and secrets@@@@@
   variables - prevent us to store hardcoded values for the pods in manifest

1) configMap variable is created for "non-sensitive" data like name of database 
2) secrets variable is use for "sensitive" data like password
    ---> to encrypt value = # echo -n pramod123 | base64
    ---> to decrypt valute = # echo cHJhbW9kMTIz | base64 -d

@@@@DaemonSet@@@
>> A DaemonSet ensures that all (or some) Nodes run a copy of a Pod.

 @@@@volume@@@
1)persistent volume (pv)
2) persistent volume claim (pvc)



         @@@@hpa@@@@

A HorizontalPodAutoscaler (HPA for short) automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand.


apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-deploy
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 30


       @@@Ingress@@@@@

*> Optimizing the use and cost of resource ingress service help us to manages external access to the services     in a cluster, typically HTTP.


# create ingress manifest file 

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  ingressClassName: nginx
  rules:
  - host: nginx.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: svc-front
            port:
              number: 80
  rules:
  - host: tomcat.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: svc-back
            port:
              number: 8080


# installed ingress controller from browser
# enter ingressclass name {kubectl get ingressclass}
# nslookup <dns>   {to convert dns into ipv4}
# vi /etc/hosts    {configured it}


H.W.  ->  purchase domain and directly access from browser

     
 Task->>
   db- statefulset
        hpa
        PV and PVC
        configMap
        secret
        headless service
   
   back - deployment  (image)
          hpa
          service
   
   front  - deployment  (image)
            hpa
            service

    Ingress manifest


>> init container
   pause container
   headless service








 